{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3dc942-cad4-49a5-abfb-8c2cf4cc8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘foreach’ was built under R version 4.2.3”\n",
      "Loading required package: iterators\n",
      "\n",
      "Warning message:\n",
      "“package ‘iterators’ was built under R version 4.2.3”\n",
      "Loading required package: parallel\n",
      "\n",
      "Loading required package: micEcon\n",
      "\n",
      "\n",
      "If you have questions, suggestions, or comments regarding one of the 'micEcon' packages, please use a forum or 'tracker' at micEcon's R-Forge site:\n",
      "https://r-forge.r-project.org/projects/micecon/\n",
      "\n",
      "Loading required package: lmtest\n",
      "\n",
      "Loading required package: zoo\n",
      "\n",
      "\n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "\n",
      "Please cite the 'frontier' package as:\n",
      "Tim Coelli and Arne Henningsen (2013). frontier: Stochastic Frontier Analysis. R package version 1.1. http://CRAN.R-Project.org/package=frontier.\n",
      "\n",
      "If you have questions, suggestions, or comments regarding the 'frontier' package, please use a forum or 'tracker' at frontier's R-Forge site:\n",
      "https://r-forge.r-project.org/projects/frontier/\n",
      "\n",
      "Loading required package: stats4\n",
      "\n",
      "\n",
      "Attaching package: ‘sn’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    sd\n",
      "\n",
      "\n",
      "Loading required package: mgcv\n",
      "\n",
      "Loading required package: nlme\n",
      "\n",
      "This is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n",
      "\n",
      "Loading required package: np\n",
      "\n",
      "Warning message:\n",
      "“package ‘np’ was built under R version 4.2.3”\n",
      "Nonparametric Kernel Methods for Mixed Datatypes (version 0.60-17)\n",
      "[vignette(\"np_faq\",package=\"np\") provides answers to frequently asked questions]\n",
      "[vignette(\"np\",package=\"np\") an overview]\n",
      "[vignette(\"entropy_np\",package=\"np\") an overview of entropy-based methods]\n",
      "\n",
      "Loading required package: gamlss\n",
      "\n",
      "Loading required package: splines\n",
      "\n",
      "Loading required package: gamlss.data\n",
      "\n",
      "\n",
      "Attaching package: ‘gamlss.data’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:datasets’:\n",
      "\n",
      "    sleep\n",
      "\n",
      "\n",
      "Loading required package: gamlss.dist\n",
      "\n",
      " **********   GAMLSS Version 5.4-3  ********** \n",
      "\n",
      "For more on GAMLSS look at https://www.gamlss.com/\n",
      "\n",
      "Type gamlssNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm(list = ls())\n",
    "#setwd('/Users/zhengwei/Desktop/OneDrive - Texas A&M University-Corpus Christi/Research/Huiyan_Sang/DecisionTree/MyPackage/sim1_lin')\n",
    "#setwd(\"/Users/zhengwei/OneDrive - Texas A&M University-Corpus Christi/Research/Huiyan_Sang/DecisionTree/MyPackage/sim1_lin\")\n",
    "\n",
    "library(foreach)\n",
    "library(doMC)\n",
    "library(coda)\n",
    "library(MonBartSFM)\n",
    "library(frontier)\n",
    "library(truncnorm)\n",
    "library(sn)\n",
    "library(MASS)\n",
    "library(semsfa)\n",
    "source(\"otherfuns.R\")\n",
    "source(\"Bayes_SFM_NHN_Gibbs.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfa84ef-70e6-43a7-9dc4-a041485f99a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>log_X</th><th scope=col>log_y</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 2.768552</td><td> 0.2527462</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-2.856968</td><td>-0.6890037</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>-1.137870</td><td> 0.5694323</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 2.452089</td><td> 2.5354790</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>-1.278954</td><td> 1.4797850</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>-1.649340</td><td> 0.2519563</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & log\\_X & log\\_y\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 &  2.768552 &  0.2527462\\\\\n",
       "\t2 & -2.856968 & -0.6890037\\\\\n",
       "\t3 & -1.137870 &  0.5694323\\\\\n",
       "\t4 &  2.452089 &  2.5354790\\\\\n",
       "\t5 & -1.278954 &  1.4797850\\\\\n",
       "\t6 & -1.649340 &  0.2519563\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | log_X &lt;dbl&gt; | log_y &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 |  2.768552 |  0.2527462 |\n",
       "| 2 | -2.856968 | -0.6890037 |\n",
       "| 3 | -1.137870 |  0.5694323 |\n",
       "| 4 |  2.452089 |  2.5354790 |\n",
       "| 5 | -1.278954 |  1.4797850 |\n",
       "| 6 | -1.649340 |  0.2519563 |\n",
       "\n"
      ],
      "text/plain": [
       "  log_X     log_y     \n",
       "1  2.768552  0.2527462\n",
       "2 -2.856968 -0.6890037\n",
       "3 -1.137870  0.5694323\n",
       "4  2.452089  2.5354790\n",
       "5 -1.278954  1.4797850\n",
       "6 -1.649340  0.2519563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set path to your folder\n",
    "path <- \"simulated_data\"\n",
    "\n",
    "# List all CSV files in that folder\n",
    "files <- list.files(path, pattern = \"\\\\.csv$\", full.names = TRUE)\n",
    "\n",
    "# Read each CSV into a list of data.frames\n",
    "simulated_datasets_R <- lapply(files, read.csv)\n",
    "\n",
    "# Access the first dataset\n",
    "head(simulated_datasets_R[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6104df-bd6e-4587-9a42-6a1902d2d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=  71 \n",
      "Cumulative average: iter= 1000 beta0= 0.8144 beta= 0.1234 sigmav= 0.8953 sigmau= 0.2522 \n",
      "Cumulative average: iter= 2000 beta0= 0.8225 beta= 0.1242 sigmav= 0.8929 sigmau= 0.2608 \n",
      "Cumulative average: iter= 3000 beta0= 0.83 beta= 0.124 sigmav= 0.8921 sigmau= 0.27 \n",
      "Cumulative average: iter= 4000 beta0= 0.8276 beta= 0.1239 sigmav= 0.893 sigmau= 0.2675 \n",
      "Cumulative average: iter= 5000 beta0= 0.8434 beta= 0.1244 sigmav= 0.8901 sigmau= 0.2867 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Rcpp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.036831, -0.500342\n",
      "first row: -1.116853, -1.116853\n",
      "second row: -1.081523, -1.081523\n",
      "last row: 2.216978, 2.216978\n",
      "no test observations\n",
      "tau: 0.145009\n",
      "nu: 3.000000\n",
      "lambda: 0.078933\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.915279 ... 2.908473\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.036831, -0.500342\n",
      "first row: -1.116853, -1.116853\n",
      "second row: -1.081523, -1.081523\n",
      "last row: 2.216978, 2.216978\n",
      "no test observations\n",
      "tau: 0.145009\n",
      "nu: 3.000000\n",
      "lambda: 0.078933\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.915279 ... 2.908473\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  72 \n",
      "Cumulative average: iter= 1000 beta0= 0.7535 beta= 0.1441 sigmav= 0.966 sigmau= 0.249 \n",
      "Cumulative average: iter= 2000 beta0= 0.7622 beta= 0.1424 sigmav= 0.9641 sigmau= 0.2606 \n",
      "Cumulative average: iter= 3000 beta0= 0.754 beta= 0.1416 sigmav= 0.9658 sigmau= 0.249 \n",
      "Cumulative average: iter= 4000 beta0= 0.7608 beta= 0.1407 sigmav= 0.9648 sigmau= 0.2562 \n",
      "Cumulative average: iter= 5000 beta0= 0.7611 beta= 0.14 sigmav= 0.9653 sigmau= 0.2568 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -0.877517, 1.387887\n",
      "first row: 2.177508, 2.177508\n",
      "second row: -2.814808, -2.814808\n",
      "last row: 2.230829, 2.230829\n",
      "no test observations\n",
      "tau: 0.156767\n",
      "nu: 3.000000\n",
      "lambda: 0.096878\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.923625 ... 2.908692\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -0.877517, 1.387887\n",
      "first row: 2.177508, 2.177508\n",
      "second row: -2.814808, -2.814808\n",
      "last row: 2.230829, 2.230829\n",
      "no test observations\n",
      "tau: 0.156767\n",
      "nu: 3.000000\n",
      "lambda: 0.096878\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.923625 ... 2.908692\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  73 \n",
      "Cumulative average: iter= 1000 beta0= 0.5923 beta= 0.2206 sigmav= 0.9544 sigmau= 0.2357 \n",
      "Cumulative average: iter= 2000 beta0= 0.6216 beta= 0.22 sigmav= 0.9505 sigmau= 0.2699 \n",
      "Cumulative average: iter= 3000 beta0= 0.6157 beta= 0.2203 sigmav= 0.952 sigmau= 0.2613 \n",
      "Cumulative average: iter= 4000 beta0= 0.6172 beta= 0.2201 sigmav= 0.9519 sigmau= 0.2641 \n",
      "Cumulative average: iter= 5000 beta0= 0.6168 beta= 0.2201 sigmav= 0.9523 sigmau= 0.2637 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 0.103884, 0.948872\n",
      "first row: 0.110813, 0.110813\n",
      "second row: 2.719336, 2.719336\n",
      "last row: 0.841384, 0.841384\n",
      "no test observations\n",
      "tau: 0.169833\n",
      "nu: 3.000000\n",
      "lambda: 0.090546\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.938848 ... 2.936938\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 0.103884, 0.948872\n",
      "first row: 0.110813, 0.110813\n",
      "second row: 2.719336, 2.719336\n",
      "last row: 0.841384, 0.841384\n",
      "no test observations\n",
      "tau: 0.169833\n",
      "nu: 3.000000\n",
      "lambda: 0.090546\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.938848 ... 2.936938\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  74 \n",
      "Cumulative average: iter= 1000 beta0= 0.7871 beta= 0.155 sigmav= 0.884 sigmau= 0.2635 \n",
      "Cumulative average: iter= 2000 beta0= 0.7984 beta= 0.1537 sigmav= 0.8834 sigmau= 0.2746 \n",
      "Cumulative average: iter= 3000 beta0= 0.7922 beta= 0.1532 sigmav= 0.8841 sigmau= 0.2668 \n",
      "Cumulative average: iter= 4000 beta0= 0.7879 beta= 0.1532 sigmav= 0.8846 sigmau= 0.2615 \n",
      "Cumulative average: iter= 5000 beta0= 0.7931 beta= 0.1531 sigmav= 0.8836 sigmau= 0.268 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.700421, -0.754977\n",
      "first row: 1.149515, 1.149515\n",
      "second row: 1.574503, 1.574503\n",
      "last row: -2.097142, -2.097142\n",
      "no test observations\n",
      "tau: 0.169150\n",
      "nu: 3.000000\n",
      "lambda: 0.054346\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.929630 ... 2.815462\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.700421, -0.754977\n",
      "first row: 1.149515, 1.149515\n",
      "second row: 1.574503, 1.574503\n",
      "last row: -2.097142, -2.097142\n",
      "no test observations\n",
      "tau: 0.169150\n",
      "nu: 3.000000\n",
      "lambda: 0.054346\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.929630 ... 2.815462\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  75 \n",
      "Cumulative average: iter= 1000 beta0= 0.7126 beta= 0.2675 sigmav= 0.9851 sigmau= 0.236 \n",
      "Cumulative average: iter= 2000 beta0= 0.713 beta= 0.2681 sigmav= 0.9839 sigmau= 0.2369 \n",
      "Cumulative average: iter= 3000 beta0= 0.7241 beta= 0.2672 sigmav= 0.9819 sigmau= 0.2492 \n",
      "Cumulative average: iter= 4000 beta0= 0.7323 beta= 0.2667 sigmav= 0.98 sigmau= 0.2587 \n",
      "Cumulative average: iter= 5000 beta0= 0.7347 beta= 0.2661 sigmav= 0.9798 sigmau= 0.262 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.410177, -0.571445\n",
      "first row: -0.812584, -0.812584\n",
      "second row: 2.505617, 2.505617\n",
      "last row: 2.443524, 2.443524\n",
      "no test observations\n",
      "tau: 0.167773\n",
      "nu: 3.000000\n",
      "lambda: 0.101407\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.872482 ... 2.940947\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.410177, -0.571445\n",
      "first row: -0.812584, -0.812584\n",
      "second row: 2.505617, 2.505617\n",
      "last row: 2.443524, 2.443524\n",
      "no test observations\n",
      "tau: 0.167773\n",
      "nu: 3.000000\n",
      "lambda: 0.101407\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.872482 ... 2.940947\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  76 \n",
      "Cumulative average: iter= 1000 beta0= 0.8244 beta= 0.239 sigmav= 0.9753 sigmau= 0.2745 \n",
      "Cumulative average: iter= 2000 beta0= 0.8533 beta= 0.2404 sigmav= 0.9674 sigmau= 0.3124 \n",
      "Cumulative average: iter= 3000 beta0= 0.8358 beta= 0.2404 sigmav= 0.9726 sigmau= 0.2898 \n",
      "Cumulative average: iter= 4000 beta0= 0.8315 beta= 0.2412 sigmav= 0.9746 sigmau= 0.284 \n",
      "Cumulative average: iter= 5000 beta0= 0.8235 beta= 0.2412 sigmav= 0.9756 sigmau= 0.2743 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.560512, -0.455999\n",
      "first row: -1.901599, -1.901599\n",
      "second row: -1.922072, -1.922072\n",
      "last row: -1.332871, -1.332871\n",
      "no test observations\n",
      "tau: 0.175241\n",
      "nu: 3.000000\n",
      "lambda: 0.083264\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.869975 ... 2.857882\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.560512, -0.455999\n",
      "first row: -1.901599, -1.901599\n",
      "second row: -1.922072, -1.922072\n",
      "last row: -1.332871, -1.332871\n",
      "no test observations\n",
      "tau: 0.175241\n",
      "nu: 3.000000\n",
      "lambda: 0.083264\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.869975 ... 2.857882\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  77 \n",
      "Cumulative average: iter= 1000 beta0= 0.7098 beta= 0.2641 sigmav= 0.9429 sigmau= 0.2495 \n",
      "Cumulative average: iter= 2000 beta0= 0.7316 beta= 0.2639 sigmav= 0.9416 sigmau= 0.2754 \n",
      "Cumulative average: iter= 3000 beta0= 0.7429 beta= 0.2634 sigmav= 0.9395 sigmau= 0.2885 \n",
      "Cumulative average: iter= 4000 beta0= 0.74 beta= 0.2634 sigmav= 0.9393 sigmau= 0.2841 \n",
      "Cumulative average: iter= 5000 beta0= 0.7662 beta= 0.2633 sigmav= 0.9346 sigmau= 0.3168 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 1.117454, -0.918803\n",
      "first row: 0.626825, 0.626825\n",
      "second row: 2.848167, 2.848167\n",
      "last row: -2.405889, -2.405889\n",
      "no test observations\n",
      "tau: 0.158129\n",
      "nu: 3.000000\n",
      "lambda: 0.073311\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.925759 ... 2.906497\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 1.117454, -0.918803\n",
      "first row: 0.626825, 0.626825\n",
      "second row: 2.848167, 2.848167\n",
      "last row: -2.405889, -2.405889\n",
      "no test observations\n",
      "tau: 0.158129\n",
      "nu: 3.000000\n",
      "lambda: 0.073311\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.925759 ... 2.906497\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  78 \n",
      "Cumulative average: iter= 1000 beta0= 0.8744 beta= 0.2052 sigmav= 0.9322 sigmau= 0.252 \n",
      "Cumulative average: iter= 2000 beta0= 0.876 beta= 0.2037 sigmav= 0.9322 sigmau= 0.2592 \n",
      "Cumulative average: iter= 3000 beta0= 0.8894 beta= 0.2035 sigmav= 0.9307 sigmau= 0.2755 \n",
      "Cumulative average: iter= 4000 beta0= 0.8819 beta= 0.2034 sigmav= 0.9317 sigmau= 0.2672 \n",
      "Cumulative average: iter= 5000 beta0= 0.8779 beta= 0.2035 sigmav= 0.9323 sigmau= 0.2621 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in semsfa(y ~ pbm(x, mono = \"up\"), sem.method = \"gam.mono\", dati):\n",
      "“the residuals of the FirstStep estimates are left skewed \n",
      " this might indicate that there is no inefficiency or that the model is misspecified”\n",
      "Warning message in sfa(y1 ~ lnx1, data = simdt):\n",
      "“the residuals of the OLS estimates are right-skewed; this might indicate that there is no inefficiency or that the model is misspecified”\n",
      "Warning message in sfa(y1 ~ lnx1, data = simdt):\n",
      "“the parameter 'gamma' is close to the boundary of the parameter space [0,1]: this can cause convergence problems and can negatively affect the validity and reliability of statistical tests and might be caused by model misspecification”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 0.435226, -1.615744\n",
      "first row: -1.601082, -1.601082\n",
      "second row: -0.885045, -0.885045\n",
      "last row: -2.234989, -2.234989\n",
      "no test observations\n",
      "tau: 0.167989\n",
      "nu: 3.000000\n",
      "lambda: 0.102087\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.878386 ... 2.927332\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: 0.435226, -1.615744\n",
      "first row: -1.601082, -1.601082\n",
      "second row: -0.885045, -0.885045\n",
      "last row: -2.234989, -2.234989\n",
      "no test observations\n",
      "tau: 0.167989\n",
      "nu: 3.000000\n",
      "lambda: 0.102087\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.878386 ... 2.927332\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  79 \n",
      "Cumulative average: iter= 1000 beta0= 0.8039 beta= 0.178 sigmav= 0.8546 sigmau= 0.2612 \n",
      "Cumulative average: iter= 2000 beta0= 0.7923 beta= 0.1781 sigmav= 0.8568 sigmau= 0.2454 \n",
      "Cumulative average: iter= 3000 beta0= 0.7975 beta= 0.1787 sigmav= 0.8566 sigmau= 0.2512 \n",
      "Cumulative average: iter= 4000 beta0= 0.7992 beta= 0.1789 sigmav= 0.8567 sigmau= 0.2527 \n",
      "Cumulative average: iter= 5000 beta0= 0.8024 beta= 0.1789 sigmav= 0.8558 sigmau= 0.2576 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.097668, 0.831905\n",
      "first row: -2.184118, -2.184118\n",
      "second row: -1.925282, -1.925282\n",
      "last row: 2.664650, 2.664650\n",
      "no test observations\n",
      "tau: 0.146706\n",
      "nu: 3.000000\n",
      "lambda: 0.075575\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.905793 ... 2.909940\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.097668, 0.831905\n",
      "first row: -2.184118, -2.184118\n",
      "second row: -1.925282, -1.925282\n",
      "last row: 2.664650, 2.664650\n",
      "no test observations\n",
      "tau: 0.146706\n",
      "nu: 3.000000\n",
      "lambda: 0.075575\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.905793 ... 2.909940\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "i=  80 \n",
      "Cumulative average: iter= 1000 beta0= 0.7332 beta= 0.1887 sigmav= 0.8372 sigmau= 0.2774 \n",
      "Cumulative average: iter= 2000 beta0= 0.7119 beta= 0.1901 sigmav= 0.8387 sigmau= 0.2535 \n",
      "Cumulative average: iter= 3000 beta0= 0.7175 beta= 0.1898 sigmav= 0.8369 sigmau= 0.2596 \n",
      "Cumulative average: iter= 4000 beta0= 0.7162 beta= 0.1898 sigmav= 0.8369 sigmau= 0.2591 \n",
      "Cumulative average: iter= 5000 beta0= 0.7152 beta= 0.1896 sigmav= 0.8373 sigmau= 0.2578 \n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.450203, -0.684547\n",
      "first row: -0.471002, -0.471002\n",
      "second row: 1.238728, 1.238728\n",
      "last row: -2.202856, -2.202856\n",
      "no test observations\n",
      "tau: 0.155444\n",
      "nu: 3.000000\n",
      "lambda: 0.070296\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.932751 ... 2.929087\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n",
      "This is my mon-Bart code\n",
      "*****Into main of monotonic bart\n",
      "**********************\n",
      "n: 200\n",
      "p: 1\n",
      "first and last y: -1.450203, -0.684547\n",
      "first row: -0.471002, -0.471002\n",
      "second row: 1.238728, 1.238728\n",
      "last row: -2.202856, -2.202856\n",
      "no test observations\n",
      "tau: 0.155444\n",
      "nu: 3.000000\n",
      "lambda: 0.070296\n",
      "tree prior base: 0.250000\n",
      "tree prior power: 0.800000\n",
      "burn (nskip): 1000\n",
      "nd (ndpost): 4000\n",
      "m (ntree): 100\n",
      "nm (mu grid size): 50\n",
      "*****nkeeptrain,nkeeptest,nkeeptestme, nkeeptreedraws: 4000, 4000, 4000, 4000\n",
      "*****printevery: 100\n",
      "*****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1\n",
      "**********************\n",
      "dip.n: 0\n",
      "x1 cuts: -2.932751 ... 2.929087\n",
      "check counts\n",
      "trcnt,tecnt,temecnt,treedrawscnt: 4000,0,0, 4000\n"
     ]
    }
   ],
   "source": [
    "#set cores for parallel computing\n",
    "RNGkind(\"L'Ecuyer-CMRG\")\n",
    "cores<-strtoi(Sys.getenv('SLURM_CPUS_PER_TASK', unset=1))\n",
    "registerDoMC(cores)\n",
    "\n",
    "part=8\n",
    "\n",
    "res<-foreach(i=71:80) %dopar% {\n",
    "  cat(\"i= \",i,'\\n')\n",
    "\n",
    "finished <- FALSE\n",
    "while(!finished) {\n",
    "    tryCatch({\n",
    "\n",
    "  ##########################################\n",
    "  #    simulate data for linear case       #\n",
    "  ##########################################\n",
    "  ## simulate simple data with one x\n",
    "  beta0=log(2)\n",
    "  beta1=0.2\n",
    "  sigmav=0.9\n",
    "  sigmau=0.2\n",
    "  nsamp=200\n",
    "\n",
    "  x_org=simulated_datasets_R[[i]]$log_X\n",
    "  y_org=simulated_datasets_R[[i]]$log_y\n",
    "\n",
    "\n",
    "  ########################################\n",
    "  # Model 1 fit the Bayes Linear Model\n",
    "  ########################################\n",
    "\n",
    "  simdt=data.frame(y1=y_org,lnx1=x_org)\n",
    "\n",
    "  B_burnin=1000\n",
    "  B_afterburnin=4000\n",
    "\n",
    "  # B_burnin=2\n",
    "  # B_afterburnin=20\n",
    "\n",
    "  res_Bayelin<-Bayeslinear_NHN_fit(B_burnin,\n",
    "                                   B_afterburnin,\n",
    "                                   simdt,\n",
    "                                   beta0_init = 0,\n",
    "                                   beta_init=beta0,\n",
    "                                   sigu_init=sigmau,\n",
    "                                   sigv_init=sigmav,\n",
    "                                   df_v = 5,\n",
    "                                   S_v =(5+2)*sigmav^2,\n",
    "                                   df_u =5,\n",
    "                                   S_u = (5+2)*sigmau^2)\n",
    "\n",
    "\n",
    "  B=B_burnin+B_afterburnin\n",
    "  res_bayes_lin<-na.omit(res_Bayelin$res[B_burnin+1:B,])\n",
    "\n",
    "  ##############################################\n",
    "  # Model 2\n",
    "  # fit SEM-SFM\n",
    "  # imposing monotonicity restrictions on inputs\n",
    "  ##############################################\n",
    "  #first-step: monotone gam, second-step: fan\n",
    "  dati=data.frame(y=y_org,x=x_org)\n",
    "  semfit<-semsfa(y~pbm(x,mono=\"up\"),sem.method = \"gam.mono\",dati)\n",
    "  sigma_v_semfit=sqrt(semfit$sigma^2/(1+semfit$lambda^2))\n",
    "  sigma_u_semfit=semfit$lambda*sigma_v_semfit\n",
    "\n",
    "  ##############################################\n",
    "  # Model 3\n",
    "  # fit Mon-BART-SFM\n",
    "  # imposing monotonicity restrictions on inputs\n",
    "  ##############################################\n",
    "  #find initial value for sigma_v and sigma_u\n",
    "\n",
    "  fit_sfa <-sfa( y1 ~ lnx1,data = simdt)\n",
    "  gamma <- unname( coef(fit_sfa)[\"gamma\"] )\n",
    "  sigmaSq <- unname( coef(fit_sfa)[\"sigmaSq\"] )\n",
    "  sigmaSqU <- gamma * sigmaSq\n",
    "  sigmaSqV <- ( 1 - gamma ) * sigmaSq\n",
    "  betavec<-coef(fit_sfa)[1:3]\n",
    "  sigma_u<-sqrt(sigmaSqU)\n",
    "  sigma_v<-sqrt(sigmaSqV)\n",
    "\n",
    "\n",
    "\n",
    "  ysnfit<-selm(y_org ~ 1, family=\"SN\")\n",
    "  center_Y <- ysnfit@param$dp[1]\n",
    "\n",
    "  nskip=B_burnin\n",
    "  ndpost=B_afterburnin\n",
    "\n",
    "\n",
    "  fitbartsfm<-monbartsfm(\n",
    "    x.train=as.matrix(x_org),y.train=y_org, x.test=matrix(0.0,0,0),\n",
    "    sigest=sigma_v,\n",
    "    sigdf=3,\n",
    "    sigquant=.95,\n",
    "    sigu_pre_est=sigma_u,\n",
    "    sigquant_u=.95,\n",
    "    k=2.0,\n",
    "    power=.8, base=.25, #regular bart is 2,.95\n",
    "    sigmaf=NA,\n",
    "    lambda=NA,\n",
    "    fmean = center_Y,\n",
    "    #fscale=scale_Y,\n",
    "    ntree=100,\n",
    "    ndpost, nskip,\n",
    "    mgsize=50,\n",
    "    nkeeptrain=ndpost,nkeeptest=ndpost,\n",
    "    nkeeptestmean=ndpost,\n",
    "    nkeeptreedraws=ndpost,\n",
    "    printevery=100\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "  fitbartsfm_beta0<-monbartsfm_beta0(\n",
    "    x.train=as.matrix(x_org),y.train=y_org, x.test=matrix(0.0,0,0),\n",
    "    sigest=sigma_v,\n",
    "    sigdf=3,\n",
    "    sigquant=.95,\n",
    "    sigu_pre_est=sigma_u,\n",
    "    sigquant_u=.95,\n",
    "    k=2.0,\n",
    "    power=.8, base=.25, #regular bart is 2,.95\n",
    "    sigmaf=NA,\n",
    "    lambda=NA,\n",
    "    fmean = center_Y,\n",
    "    #fscale=scale_Y,\n",
    "    ntree=100,\n",
    "    ndpost, nskip,\n",
    "    mgsize=50,\n",
    "    nkeeptrain=ndpost,nkeeptest=ndpost,\n",
    "    nkeeptestmean=ndpost,\n",
    "    nkeeptreedraws=ndpost,\n",
    "    printevery=100\n",
    "  )\n",
    "\n",
    "  gtest<-geweke.diag(fitbartsfm$sigma_u)\n",
    "  pval_BART<-1-pnorm(gtest$z)\n",
    "  # pval_BART=0.5\n",
    "  gtest_b0<-geweke.diag(fitbartsfm_beta0$sigma_u)\n",
    "  pval_BART_b0<-1-pnorm(gtest_b0$z)\n",
    "  # pval_BART_b0=0.5\n",
    "  finished <- TRUE #if convergence test is passed, then next iteration\n",
    "\n",
    "  if(pval_BART>0.1&pval_BART_b0>0.1){\n",
    "    finished <- TRUE #if convergence test is passed, then next iteration\n",
    "  }\n",
    "\n",
    "  #######################################\n",
    "  # 1. Compare RMSE and BIAS            #\n",
    "  #######################################\n",
    "\n",
    "  y_tf = exp(beta0)*exp(x_org)^beta1 #((X**w_true)*b_true).numpy() \n",
    "  #############################################################################\n",
    "  #formulas are from Eq. 10 and 11 from                                       #\n",
    "  #Ferrara and Vidoli 2017 Semiparametric Stochastic frontier models          #\n",
    "  #############################################################################\n",
    "    \n",
    "  RMSE_bayeslin           = 0 #mean(((mean(res_Bayelin$res$beta0_post)+mean(res_Bayelin$res$beta_current)*x_org-y_org)/y_org)^2) # I did not calculate the Bayes lin\n",
    "  # e_sem = y_org - (semfit$fitted)\n",
    "  # E_sem_correction = mean(exp(e_sem))\n",
    "\n",
    "  E_sem_correction = exp((sigma_v_semfit**2+sigma_u_semfit**2)/2)    \n",
    "  RMSE_sem         = mean(((exp(semfit$fitted)*E_sem_correction-y_tf)/y_tf)^2)\n",
    "  # e_bartsfm = y_org - (fitbartsfm$yhat.train.mean)\n",
    "  # E_bartsfm_correction = mean(exp(e_bartsfm))\n",
    "  sigv_map = median(fitbartsfm$sigma[(nskip+1):(nskip+ndpost)])\n",
    "  sigu_map = median(fitbartsfm$sigma_u[(nskip+1):(nskip+ndpost)])\n",
    "  \n",
    "  E_bartsfm_correction =  exp((sigv_map**2+sigv_map**2)/2)    \n",
    "  RMSE_bartsfm            =mean(((exp(fitbartsfm$yhat.train.mean)*E_bartsfm_correction-y_tf)/y_tf)^2)\n",
    "        \n",
    "  # RMSE_bartsfm_uadj       =mean(((fitbartsfm_uadj$yhat.train.mean-y_org)/y_org)^2)\n",
    "  # e_bartsfm_beta0 = y_org - (fitbartsfm_beta0$yhat.train.mean)\n",
    "  # E_bartsfm_beta0_correction = mean(exp(e_bartsfm_beta0))\n",
    "  sigv_beta0_map = median(fitbartsfm_beta0$sigma[(nskip+1):(nskip+ndpost)])\n",
    "  sigu_beta0_map = median(fitbartsfm_beta0$sigma_u[(nskip+1):(nskip+ndpost)])\n",
    "\n",
    "  E_bartsfm_beta0_correction = exp((sigv_beta0_map**2+sigu_beta0_map**2)/2)   \n",
    "  RMSE_bartsfm_beta0      =mean(((exp(fitbartsfm_beta0$yhat.train.mean)*E_bartsfm_beta0_correction-y_tf)/y_tf)^2)\n",
    "  # RMSE_bartsfm_beta0_uadj =mean(((fitbartsfm_beta0_uadj$yhat.train.mean-y_org)/y_org)^2)\n",
    "\n",
    "  BIAS_bayeslin           =mean(abs((mean(res_Bayelin$res$beta0_post)+mean(res_Bayelin$res$beta_current)*x_org-y_org)/y_org))\n",
    "  BIAS_sem                =mean(abs((exp(semfit$fitted)*E_sem_correction-y_tf)/y_tf))\n",
    "  BIAS_bartsfm            =mean(abs((exp(fitbartsfm$yhat.train.mean)*E_bartsfm_correction-y_tf)/y_tf))\n",
    "  # BIAS_bartsfm_uadj       =median(((fitbartsfm_uadj$yhat.train.mean-y_org)/y_org))\n",
    "  BIAS_bartsfm_beta0      =median(abs((exp(fitbartsfm_beta0$yhat.train.mean)*E_bartsfm_beta0_correction-y_tf)/y_tf))\n",
    "  # BIAS_bartsfm_beta0_uadj =median(((fitbartsfm_beta0_uadj$yhat.train.mean-y_org)/y_org))\n",
    "\n",
    "  RMSE_res<-c(RMSE_bayeslin,\n",
    "              RMSE_sem,\n",
    "              RMSE_bartsfm,\n",
    "            # RMSE_bartsfm_uadj,\n",
    "              RMSE_bartsfm_beta0\n",
    "  )\n",
    "\n",
    "  BIAS_res <- c(BIAS_bayeslin,\n",
    "                BIAS_sem,\n",
    "                BIAS_bartsfm,\n",
    "                # BIAS_bartsfm_uadj,\n",
    "                BIAS_bartsfm_beta0\n",
    "                # BIAS_bartsfm_beta0_uadj\n",
    "                )\n",
    "\n",
    "  ##############################################################################\n",
    "  # 2. Compare mean bias estimates of sigma_u and sigma_v                           #\n",
    "  ##############################################################################\n",
    "  Bias_sig_v_bayeslin           = abs(median(res_bayes_lin$sigv_post)-sigmav)\n",
    "  Bias_sig_v_sem                = abs(sigma_v_semfit-sigmav)\n",
    "  Bias_sig_v_bartsfm            = abs(median(fitbartsfm$sigma[(nskip+1):(nskip+ndpost)])      -sigmav)\n",
    "  # Bias_sig_v_bartsfm_uadj       = abs(median(fitbartsfm_uadj$sigma[(nskip+1):(nskip+ndpost)]) -sigmav)\n",
    "  Bias_sig_v_bartsfm_beta0      = abs(median(fitbartsfm_beta0$sigma[(nskip+1):(nskip+ndpost)])-sigmav)\n",
    "  # Bias_sig_v_bartsfm_beta0_uadj = abs(median(fitbartsfm_uadj$sigma[(nskip+1):(nskip+ndpost)]) -sigmav)\n",
    "\n",
    "  Bias_sig_u_bayeslin           = abs(median(res_bayes_lin$sigu_post)-sigmau)\n",
    "  Bias_sig_u_sem                = abs(sigma_u_semfit-sigmau)\n",
    "  Bias_sig_u_bartsfm            = abs(median(fitbartsfm$sigma_u[(nskip+1):(nskip+ndpost)])     -sigmau)\n",
    "  # Bias_sig_u_bartsfm_uadj       = abs(median(fitbartsfm_uadj$sigma_u[(nskip+1):(nskip+ndpost)])  -sigmau)\n",
    "  Bias_sig_u_bartsfm_beta0      = abs(median(fitbartsfm_beta0$sigma_u[(nskip+1):(nskip+ndpost)])  -sigmau)\n",
    "  # Bias_sig_u_bartsfm_beta0_uadj = abs(median(fitbartsfm_uadj$sigma_u[(nskip+1):(nskip+ndpost)])   -sigmau)\n",
    "\n",
    "  Bias_sig_v_res <- c(\n",
    "      Bias_sig_v_bayeslin      ,\n",
    "      Bias_sig_v_sem           ,\n",
    "      Bias_sig_v_bartsfm       ,\n",
    "      # Bias_sig_v_bartsfm_uadj  ,\n",
    "      Bias_sig_v_bartsfm_beta0\n",
    "      # Bias_sig_v_bartsfm_beta0_uadj\n",
    "    )\n",
    "\n",
    "  Bias_sig_u_res <-\n",
    "    c(Bias_sig_u_bayeslin      ,\n",
    "      Bias_sig_u_sem           ,\n",
    "      Bias_sig_u_bartsfm       ,\n",
    "      # Bias_sig_u_bartsfm_uadj  ,\n",
    "      Bias_sig_u_bartsfm_beta0\n",
    "      # Bias_sig_u_bartsfm_beta0_uadj\n",
    "    )\n",
    "\n",
    "\n",
    "  ##############################################################################\n",
    "  # 3. Compare mean inefficiency scores                                        #\n",
    "  ##############################################################################\n",
    "  #True TEs\n",
    "  res_true<-simdt$y1-(beta0+beta1*simdt$lnx1)\n",
    "  TEvec_true<-sapply(res_true,TE_fun,sigv=sigmav,sigu=sigmau)\n",
    "  mean(TEvec_true)\n",
    "\n",
    "  #Bayes regression TEs\n",
    "  TEvec_bayeslin<-apply(res_bayes_lin,1,function(x){ return(TE_bayes_fun(bayelinres = x,y = simdt$y1,x=simdt$lnx1))})\n",
    "  mean(TEvec_bayeslin)\n",
    "\n",
    "  #TE for sem\n",
    "  res_sem<-simdt$y1-semfit$fitted\n",
    "  TEvec_sem<-sapply(res_sem,TE_fun,sigv=sigma_v_semfit,sigu=sigma_u_semfit)\n",
    "  mean(TEvec_sem)\n",
    "\n",
    "  #TE for bart-sfm\n",
    "  TEvec_bartsfm<-apply(cbind(fitbartsfm$yhat.train,\n",
    "                             fitbartsfm$sigma[(nskip+1):(nskip+ndpost)],\n",
    "                             fitbartsfm$sigma_u[(nskip+1):(nskip+ndpost)]),1,\n",
    "                       function(y){return(TE_bart_fun(yhat_post = y[1:nsamp],\n",
    "                                                      sigv_post = y[nsamp+1],\n",
    "                                                      sigu_post = y[nsamp+2],\n",
    "                                                      y_bart = simdt$y1\n",
    "                       ))})\n",
    "\n",
    "  #TE for bart-sfm-uadj\n",
    "  # TEvec_bartsfm_uadj<-apply(cbind(fitbartsfm_uadj$yhat.train,\n",
    "  #                                 fitbartsfm_uadj$sigma[(nskip+1):(nskip+ndpost)],\n",
    "  #                                 fitbartsfm_uadj$sigma_u[(nskip+1):(nskip+ndpost)]),1,\n",
    "  #                           function(y){return(TE_bart_fun(yhat_post = y[1:nsamp],\n",
    "  #                                                          sigv_post = y[nsamp+1],\n",
    "  #                                                          sigu_post = y[nsamp+2],\n",
    "  #                                                          y_bart = simdt$y1\n",
    "  #                           ))})\n",
    "\n",
    "  #TE for bart-sfm-beta0\n",
    "  TEvec_bartsfm_beta0<-apply(cbind(fitbartsfm_beta0$yhat.train,\n",
    "                                   fitbartsfm_beta0$sigma[(nskip+1):(nskip+ndpost)],\n",
    "                                   fitbartsfm_beta0$sigma_u[(nskip+1):(nskip+ndpost)]),1,\n",
    "                             function(y){return(TE_bart_fun(yhat_post = y[1:nsamp],\n",
    "                                                            sigv_post = y[nsamp+1],\n",
    "                                                            sigu_post = y[nsamp+2],\n",
    "                                                            y_bart = simdt$y1\n",
    "                             ))})\n",
    "\n",
    "\n",
    "  #TE for bart-sfm-beta0-uadj\n",
    "  # TEvec_bartsfm_beta0_uadj<-apply(cbind(fitbartsfm_beta0_uadj$yhat.train,\n",
    "  #                                       fitbartsfm_beta0_uadj$sigma[(nskip+1):(nskip+ndpost)],\n",
    "  #                                       fitbartsfm_beta0_uadj$sigma_u[(nskip+1):(nskip+ndpost)]),1,\n",
    "  #                                 function(y){return(TE_bart_fun(yhat_post = y[1:nsamp],\n",
    "  #                                                                sigv_post = y[nsamp+1],\n",
    "  #                                                                sigu_post = y[nsamp+2],\n",
    "  #                                                                y_bart = simdt$y1\n",
    "  #                                 ))})\n",
    "\n",
    "  TE_bias<-abs(c(\n",
    "    mean(TEvec_bayeslin),\n",
    "    mean(TEvec_sem),\n",
    "    mean(TEvec_bartsfm),\n",
    "    # mean(TEvec_bartsfm_uadj),\n",
    "    mean(TEvec_bartsfm_beta0)\n",
    "    # mean(TEvec_bartsfm_beta0_uadj)\n",
    "  )-mean(TEvec_true))\n",
    "\n",
    "    },\n",
    "  error = function(e) {})\n",
    "}\n",
    "\n",
    "  # TE_bias<-rbind(TE_bias,TE_bias_one)\n",
    "\n",
    "  return(list(RMSE=RMSE_res,\n",
    "              BIAS=BIAS_res,\n",
    "              Bias_sig_v=Bias_sig_v_res,\n",
    "              Bias_sig_u=Bias_sig_u_res,\n",
    "              TE_bias=TE_bias,\n",
    "              simdata=simdt,\n",
    "              sig_v_bart_post=fitbartsfm$sigma,\n",
    "              sig_u_bart_post=fitbartsfm$sigma_u,\n",
    "              sig_v_bart_b0_post=fitbartsfm_beta0$sigma,\n",
    "              sig_u_bart_b0_post=fitbartsfm_beta0$sigma_u\n",
    "              ))\n",
    "\n",
    "}\n",
    "\n",
    "RMSE_res<-as.data.frame(do.call(rbind,lapply(res,function(x){x[[1]]})))\n",
    "colnames(RMSE_res)<-c(\"RMSE_BayesLin\",\"RMSE_Sem\",\n",
    "                      \"RMSE_Bart\",\n",
    "                      # \"RMSE_Bart_uadj\",\n",
    "                      \"RMSE_Bart_b0\"\n",
    "                      # \"RMSE_Bart_uadj_b0\"\n",
    ")\n",
    "\n",
    "BIAS_res<-as.data.frame(do.call(rbind,lapply(res,function(x){x[[2]]})))\n",
    "colnames(BIAS_res)<-c(\"BIAS_BayesLin\",\"BIAS_Sem\",\n",
    "                      \"BIAS_Bart\",\n",
    "                      # \"BIAS_Bart_uadj\",\n",
    "                      \"BIAS_Bart_b0\"\n",
    "                      # \"BIAS_Bart_uadj_b0\"\n",
    ")\n",
    "\n",
    "Bias_sig_v_res<-as.data.frame(do.call(rbind,lapply(res,function(x){x[[3]]})))\n",
    "colnames(Bias_sig_v_res)<-c(\"Sigvb_BayesLin\",\"Sigvb_Sem\",\n",
    "                            \"Sigvb_Bart\",\n",
    "                            # \"Sigvb_Bart_uadj\",\n",
    "                            \"Sigvb_Bart_b0\"\n",
    "                            # \"Sigvb_Bart_uadj_b0\"\n",
    ")\n",
    "\n",
    "Bias_sig_u_res<-as.data.frame(do.call(rbind,lapply(res,function(x){x[[4]]})))\n",
    "colnames(Bias_sig_u_res)<-c(\"Sigub_BayesLin\",\n",
    "                            \"Sigub_Sem\",\n",
    "                            \"Sigub_Bart\",\n",
    "                            # \"Sigub_Bart_uadj\",\n",
    "                            \"Sigub_Bart_b0\"\n",
    "                            # \"Sigub_Bart_uadj_b0\"\n",
    ")\n",
    "\n",
    "TE_bias<-as.data.frame(do.call(rbind,lapply(res,function(x){x[[5]]})))\n",
    "colnames(TE_bias)<-c(\"TEb_BayesLin\",\"TEb_Sem\",\n",
    "                     \"TEb_Bart\",\n",
    "                     # \"TEb_Bart_uadj\",\n",
    "                     \"TEb_Bart_b0\"\n",
    "                     # \"TEb_Bart_uadj_b0\"\n",
    ")\n",
    "\n",
    "resfin<-list(RMSE=RMSE_res,\n",
    "             BIAS=BIAS_res,\n",
    "             Bias_sigv=Bias_sig_v_res,\n",
    "             Bias_sigu=Bias_sig_u_res,\n",
    "             TE_bias=TE_bias,\n",
    "             res=res)\n",
    "saveRDS(resfin, paste(\"res/linear_case_part\",part,\".rds\",sep = \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R_monbart",
   "language": "R",
   "name": "r4.1.2_monbart"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
