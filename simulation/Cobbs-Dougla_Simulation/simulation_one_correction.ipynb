{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b3fd81-b596-4dd2-af9e-d8fa45166f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with ReLU activation...\n",
      "\n",
      "Training model with ELU activation...\n",
      "\n",
      "Training model with FlippedReLU activation...\n",
      "\n",
      "Training model with FlippedELU activation...\n",
      "Training completed for all activations.\n"
     ]
    }
   ],
   "source": [
    "# Clear all variables\n",
    "%reset -f\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "from torch.distributions import Normal\n",
    "import nnwosd as wosd\n",
    "import importlib\n",
    "importlib.reload(wosd)\n",
    "\n",
    "#load function for fitting SFM by mle\n",
    "from sfm_mle import estimate\n",
    "\n",
    "\n",
    "n_samples=200\n",
    "\n",
    "# True weights and bias\n",
    "b_true = 2\n",
    "w_true = torch.tensor([.2])\n",
    "\n",
    "# Generate output with Gaussian noise\n",
    "noise_std_v = .9\n",
    "noise_std_u = .2\n",
    "\n",
    "noise_v = torch.from_numpy(np.random.normal(0, noise_std_v, size=(n_samples, 1)).astype(np.float32))\n",
    "noise_u = torch.from_numpy(abs(np.random.normal(0, noise_std_u, size=(n_samples, 1)).astype(np.float32)))\n",
    "#case 1: linear\n",
    "\n",
    "X = torch.from_numpy((np.random.uniform(0.01, 3, size=(n_samples, 1)).astype(np.float32)))\n",
    "log_X = torch.log(X)\n",
    "\n",
    "# y =  (X ** w_true) * b_true * np.exp( noise_v - noise_u)  # y = 5x + 10 + N(0, 2) - HN(0, 0.5)\n",
    "log_y = w_true * log_X + np.log(b_true) + (noise_v - noise_u)\n",
    "\n",
    "\n",
    "#fit linear SFM\n",
    "\n",
    "# Import Data\n",
    "y = log_y.numpy().flatten()  # Output\n",
    "x1 = log_X.numpy().flatten()\n",
    "\n",
    "\n",
    "true_vals = [b_true, w_true, noise_std_u, noise_std_v]\n",
    "coefs, sterr, logMLE = estimate(y, x1, b_true, w_true, noise_std_u, noise_std_v)\n",
    "\n",
    "\n",
    "# fit nn sfm with 4 different activation functions\n",
    "\n",
    "\n",
    "# 2. Standardize input and output data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit scalers on the data and transform\n",
    "X_standardized = torch.tensor(scaler_X.fit_transform(log_X), dtype=torch.float32)\n",
    "y_standardized = torch.tensor(scaler_y.fit_transform(log_y), dtype=torch.float32)\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 1  # Number of input features\n",
    "hidden_sizes = [32,8]  # Concave-Relu Hidden layer sizes\n",
    "output_size = 1 # \n",
    "\n",
    "# Variables to track the best loss and model parameters\n",
    "best_loss = float('inf')  # Initialize best loss to infinity\n",
    "best_model_state = None    # To save the model state\n",
    "\n",
    "# List to store loss values for plotting\n",
    "loss_values = []\n",
    "\n",
    "# Train the deep learning model by minimizing the NLL loss\n",
    "epochs = 1000\n",
    "\n",
    "# activation_fun = nn.ReLU()\n",
    "# activation_fun = nn.ELU()\n",
    "# activation_fun = wosd.FlippedLeakRELU(alpha=0.8)\n",
    "# activation_fun = wosd.FlippedELU(alpha=0.8)\n",
    "\n",
    "activations = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"ELU\": nn.ELU(),\n",
    "    \"FlippedReLU\": wosd.FlippedLeakRELU(alpha=0.8),\n",
    "    \"FlippedELU\": wosd.FlippedELU(alpha=0.8)\n",
    "}\n",
    "\n",
    "# Which activations need weight clamping\n",
    "clamp_activations = [\"FlippedReLU\", \"FlippedELU\"]\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for name, activation_fun in activations.items():\n",
    "    print(f\"\\nTraining model with {name} activation...\")\n",
    "\n",
    "    # Instantiate model and loss\n",
    "    model = wosd.MLP(input_size, hidden_sizes, output_size, activation_func=activation_fun)\n",
    "    nll_loss = wosd.GaussianNLLLoss(sigma_v=noise_std_v, sigma_u=noise_std_u)\n",
    "    optimizer = optim.Adam(list(model.parameters()) + [nll_loss.log_std_v, nll_loss.log_std_u], lr=0.01)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_pred = model(X_standardized)\n",
    "        loss = nll_loss(y_pred, y_standardized)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Apply weight constraints for self-defined activations\n",
    "        if name in clamp_activations:\n",
    "            with torch.no_grad():\n",
    "                for layer in model.layers:\n",
    "                    layer.weight.data.clamp_(min=0)\n",
    "                model.output.weight.clamp_(min=0)\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save everything in dictionary\n",
    "    fitted_models[name] = {\n",
    "        \"model\": model,\n",
    "        \"loss_history\": loss_values,\n",
    "        \"std_v_est\": torch.exp(nll_loss.log_std_v).item() * scaler_y.scale_,\n",
    "        \"std_u_est\": torch.exp(nll_loss.log_std_u).item() * scaler_y.scale_\n",
    "    }\n",
    "\n",
    "print(\"Training completed for all activations.\")\n",
    "\n",
    "# Get all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18bc9339-5bf7-4f0d-98d1-f88ba1ae1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_sfm = log_y - (coefs[0] + coefs[1] * log_X)\n",
    "\n",
    "# e_sfm_np = e_sfm.detach().numpy()   # detach from graph, convert to NumPy\n",
    "# E_sfm_correction = np.mean(np.exp(e_sfm_np))\n",
    "\n",
    "# y_sfm_mean = ((X.numpy() ** coefs[1]) * np.exp(coefs[0])) * E_sfm_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2b3efd-4110-45cd-a8cc-58a8610b63be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN (ReLU): RMSE = 2.7517, BIAS = 2.6676, bias_v_nn = 0.0561, bias_u_nn = 0.0125,TE_bias_nn = 0.0270\n",
      "NN (ELU): RMSE = 2.7862, BIAS = 2.6949, bias_v_nn = 0.0528, bias_u_nn = 0.0117,TE_bias_nn = 0.0267\n",
      "NN (FlippedReLU): RMSE = 2.6901, BIAS = 2.6888, bias_v_nn = 0.0360, bias_u_nn = 0.0080,TE_bias_nn = 0.0243\n",
      "NN (FlippedELU): RMSE = 2.7006, BIAS = 2.6901, bias_v_nn = 0.0375, bias_u_nn = 0.0083,TE_bias_nn = 0.0246\n"
     ]
    }
   ],
   "source": [
    "res_tf = log_y - (b_true + w_true * log_X)\n",
    "e_tf_np = res_tf.detach().numpy()\n",
    "E_tf_correction=np.mean(np.exp(e_tf_np))\n",
    "\n",
    "y_tf = ((X**w_true)*b_true).numpy()*E_tf_correction\n",
    "\n",
    "e_sfm = log_y - (coefs[0] + coefs[1] * log_X)\n",
    "\n",
    "e_sfm_np = e_sfm.detach().numpy()   # detach from graph, convert to NumPy\n",
    "E_sfm_correction = np.mean(np.exp(e_sfm_np))\n",
    "\n",
    "y_sfm_mean = ((X.numpy() ** coefs[1]) * np.exp(coefs[0])) * E_sfm_correction\n",
    "\n",
    "RMSE_sfm = (np.mean(((y_sfm_mean  -y_tf)/y_tf)**2))\n",
    "BIAS_sfm = np.mean(np.abs(y_sfm_mean -y_tf/y_tf))\n",
    "\n",
    "sigma_v_sfm = np.sqrt(coefs[3])\n",
    "sigma_u_sfm = np.sqrt(coefs[2])\n",
    "\n",
    "bias_v_sfm = np.abs(sigma_v_sfm-noise_std_v)\n",
    "bias_u_sfm = np.abs(sigma_u_sfm-noise_std_u)\n",
    "\n",
    "#TE\n",
    "vectorized_TE_fun = np.vectorize(wosd.TE_fun)\n",
    "\n",
    "TE_true = vectorized_TE_fun(residuals=((np.log(y_tf)-(w_true * log_X + np.log(b_true)).numpy())),sig_v=noise_std_v,sig_u=noise_std_u)\n",
    "TE_sfm = vectorized_TE_fun(residuals=(np.log(y_sfm_mean)-log_y.numpy()),sig_v=sigma_v_sfm,sig_u=sigma_u_sfm)\n",
    "\n",
    "Bias_TE_sfm = np.mean(np.abs(TE_sfm-TE_true))\n",
    "\n",
    "# RMSE for NN models\n",
    "RMSE_nn = {}\n",
    "BIAS_nn = {}\n",
    "bias_v_nn = {}\n",
    "bias_u_nn = {}\n",
    "Bias_TE_nn = {}\n",
    "\n",
    "for name, info in fitted_models.items():\n",
    "    model = info['model']\n",
    "    with torch.no_grad():\n",
    "        y_pred_std = model(X_standardized)\n",
    "        # Add inefficiency correction if needed\n",
    "        std_u_est = info['std_u_est']\n",
    "        y_pred_std = y_pred_std + np.sqrt(2/np.pi)*std_u_est\n",
    "        # Inverse transform to original scale\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred_std.numpy())\n",
    "        \n",
    "        y_original = scaler_y.inverse_transform(y_standardized.numpy())\n",
    "        residuals_nn= y_original - y_pred_original\n",
    "        E_nn_correction = np.mean(np.exp(residuals_nn))\n",
    "        \n",
    "    \n",
    "    # Calculate RMSE relative to true values\n",
    "    rmse = np.sqrt(np.mean(((np.exp(y_pred_original)*E_nn_correction - y_tf)/y_tf)**2))\n",
    "    RMSE_nn[name] = rmse\n",
    "    # Calculate absolute relative bias\n",
    "    bias = np.mean(np.abs((np.exp(y_pred_original)*E_nn_correction - y_tf)/y_tf))\n",
    "    BIAS_nn[name] = bias\n",
    "    \n",
    "    sigma_v_nn=fitted_models[name][\"std_v_est\"]\n",
    "    sigma_u_nn=fitted_models[name][\"std_u_est\"]\n",
    "    \n",
    "    bias_v_nn[name] = np.abs(sigma_v_nn - noise_std_v).item()\n",
    "    bias_u_nn[name] = np.abs(sigma_u_nn - noise_std_u).item()\n",
    "    \n",
    "    # Calculate the bias of TE\n",
    "    TE_nn = vectorized_TE_fun(residuals=(y_pred_original-log_y.numpy()),sig_v=sigma_v_nn,sig_u=sigma_u_nn)\n",
    "    Bias_TE_nn[name] = np.mean(np.abs(TE_nn-TE_true)) \n",
    "    \n",
    "    print(f\"NN ({name}): RMSE = {rmse:.4f}, BIAS = {bias:.4f}, bias_v_nn = {bias_v_nn[name]:.4f}, bias_u_nn = {bias_u_nn[name]:.4f},TE_bias_nn = {Bias_TE_nn[name]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "RMSE_nn['sfm']   =float(RMSE_sfm) \n",
    "BIAS_nn['sfm']   =float(BIAS_sfm   ) \n",
    "bias_v_nn['sfm'] =float(bias_v_sfm ) \n",
    "bias_u_nn['sfm'] =float(bias_u_sfm )  \n",
    "Bias_TE_nn['sfm']=float(Bias_TE_sfm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddfe7b-43d8-4e1e-bdbb-b44cb29f33f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
